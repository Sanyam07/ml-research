name: metalearn_binary
experiment_type: METALEARN_REINFORCE
created_at: 2019-10-02-01-21-54
git_hash: b59eafd
parameters:
  datasets: null
  output_fp: /output
  n_trials: 1
  input_size: 256
  hidden_size: 256
  output_size: 256
  n_layers: 5
  dropout_rate: 0.3
  beta: 0.9
  entropy_coef: 1.0
  entropy_coef_anneal_to: 0.2
  entropy_coef_anneal_by: 0.9
  with_baseline: true
  single_baseline: false
  normalize_reward: true
  n_episodes: 250
  n_iter: 100
  n_eval_iter: 100
  learning_rate: 0.005
  env_sources:
  - AUTOSKLEARN_BENCHMARK
  - OPEN_ML
  test_env_sources:
  - OPEN_ML_BENCHMARK_CC18
  - KAGGLE
  target_types:
  - BINARY
  test_env_target_types:
  - BINARY
  - MULTICLASS
  - REGRESSION
  test_set_config:
    SKLEARN:
      test_size: 0.2
      random_state: 100
    OPEN_ML:
      test_size: 0.2
      random_state: 100
    KAGGLE:
      test_size: 0.2
      random_state: 100
  error_reward: 0
  n_samples: 10000
  per_framework_time_limit: 360
  per_framework_memory_limit: 5000
  metric_logger: floyd
  fit_verbose: 1
  controller_seed: 1000
  task_environment_seed: 100
description: |
  # test metalearning capabilities
  * train agent on binary tasks
  * evaluation plan: test against all task held out test sets
  * evaluation plan: test against MULTICLASS, REGRESSION tasks
  * evaluation plan: test against AUTOSKLEARN_BENCHMARK tasks

  * descrease entropy coefficient to reduce exploration
  * normalize rewards, maintain one baseline reward function per task env
  * increase entropy coefficient to encourage exploration
  * increase number of episodes and iterations per episode
  * test results with entropy annealing
  * bugfix: exclusion mask logic in controller class led to conflicting masks
  * increase number of episodes, add entropy annealing
  * add entropy annealing to argument and number of eval iterations

  * action activation grad is persisted across fit iterations
  * use multclass and regression for test env target types
  * increase entropy
  * increase entropy anneal to parameter
